{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "tf.set_random_seed(7)\n",
    "np.random.seed(7)\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from scipy.special import expit\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "import os\n",
    "from tensorflow.contrib.rnn import LSTMCell as Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/srikanth_sarma/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/srikanth_sarma/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/srikanth_sarma/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train_stories.csv')\n",
    "val = pd.read_csv('data/cloze_test_val__spring2016 - cloze_test_ALL_val.csv')\n",
    "test = pd.read_csv('data/cloze_test_test__spring2016 - cloze_test_ALL_test.csv')\n",
    "\n",
    "train = train.drop(\"storytitle\", axis=1).drop(\"storyid\", axis=1)\n",
    "\n",
    "val = val.drop(\"InputStoryid\", axis=1)\n",
    "val_answer = val[\"AnswerRightEnding\"]\n",
    "val_sentences = val.drop(\"AnswerRightEnding\", axis=1)\n",
    "\n",
    "test = test.drop(\"InputStoryid\", axis=1)\n",
    "test_answer = test[\"AnswerRightEnding\"]\n",
    "test_sentences = test.drop(\"AnswerRightEnding\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>sentence3</th>\n",
       "      <th>sentence4</th>\n",
       "      <th>sentence5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kelly found her grandmother's pizza recipe in ...</td>\n",
       "      <td>Kelly reminisced about how much she loved her ...</td>\n",
       "      <td>Kelly decided that she was going to try to mak...</td>\n",
       "      <td>Kelly studied the recipe and gathered everythi...</td>\n",
       "      <td>Kelly successfully made a pizza from her grand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Leo wore a toupee and was anxious about it.</td>\n",
       "      <td>He decided to go out for a short walk.</td>\n",
       "      <td>It was a very windy day, but he wasn't too con...</td>\n",
       "      <td>Suddenly, a strong wind came through and took ...</td>\n",
       "      <td>His dog leaped and caught it, and he quickly r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jimmy was a master with his grill.</td>\n",
       "      <td>He spent every weekend getting better with his...</td>\n",
       "      <td>One day he was offered a TV show about grillin...</td>\n",
       "      <td>Jimmy accepted the job in an instant.</td>\n",
       "      <td>He quit his day job and spent all his time gri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tom and Beth were dating.</td>\n",
       "      <td>Tom wanted to go on a fun date.</td>\n",
       "      <td>Tom suggested ice skating.</td>\n",
       "      <td>His date was excited and they went ice skating.</td>\n",
       "      <td>Tom and Beth grew closer during their ice skat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I had a friend that I didn't know well but let...</td>\n",
       "      <td>She paid rent then asked for some of it back.</td>\n",
       "      <td>She drinks my juice and eats my food.</td>\n",
       "      <td>She also makes a huge mess and is very sloppy.</td>\n",
       "      <td>She got kicked out in two weeks.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence1  \\\n",
       "0  Kelly found her grandmother's pizza recipe in ...   \n",
       "1        Leo wore a toupee and was anxious about it.   \n",
       "2                 Jimmy was a master with his grill.   \n",
       "3                          Tom and Beth were dating.   \n",
       "4  I had a friend that I didn't know well but let...   \n",
       "\n",
       "                                           sentence2  \\\n",
       "0  Kelly reminisced about how much she loved her ...   \n",
       "1             He decided to go out for a short walk.   \n",
       "2  He spent every weekend getting better with his...   \n",
       "3                    Tom wanted to go on a fun date.   \n",
       "4      She paid rent then asked for some of it back.   \n",
       "\n",
       "                                           sentence3  \\\n",
       "0  Kelly decided that she was going to try to mak...   \n",
       "1  It was a very windy day, but he wasn't too con...   \n",
       "2  One day he was offered a TV show about grillin...   \n",
       "3                         Tom suggested ice skating.   \n",
       "4              She drinks my juice and eats my food.   \n",
       "\n",
       "                                           sentence4  \\\n",
       "0  Kelly studied the recipe and gathered everythi...   \n",
       "1  Suddenly, a strong wind came through and took ...   \n",
       "2              Jimmy accepted the job in an instant.   \n",
       "3    His date was excited and they went ice skating.   \n",
       "4     She also makes a huge mess and is very sloppy.   \n",
       "\n",
       "                                           sentence5  \n",
       "0  Kelly successfully made a pizza from her grand...  \n",
       "1  His dog leaped and caught it, and he quickly r...  \n",
       "2  He quit his day job and spent all his time gri...  \n",
       "3  Tom and Beth grew closer during their ice skat...  \n",
       "4                   She got kicked out in two weeks.  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "440805"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13097"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stories_as_lists(dataframe):\n",
    "    a = time.time()\n",
    "    print(\"Reading stories to memory ...\")\n",
    "    stories = []\n",
    "    stories_flat = []\n",
    "    for index, row in dataframe.iterrows():\n",
    "        story = []\n",
    "        for col in dataframe.columns:\n",
    "            story.append(word_tokenize(row[col]))\n",
    "        stories.append(story)\n",
    "    print(\"Done in {} s\".format(time.time() - a))\n",
    "    return stories\n",
    "\n",
    "def sentence_sentiment(sentence, sentiment_analyzer):\n",
    "    n_positive = 0\n",
    "    n_negative = 0\n",
    "#     sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "    for word in sentence:\n",
    "        sentiment = sentiment_analyzer.polarity_scores(word)\n",
    "        if sentiment['pos']>sentiment['neg'] and sentiment['pos']>sentiment['neu']:\n",
    "            n_positive+=1\n",
    "        elif sentiment['neg']>sentiment['pos'] and sentiment['neg']>sentiment['neu']:\n",
    "            n_negative+=1\n",
    "        else:\n",
    "            continue\n",
    "    return np.sign(n_positive-n_negative)+1\n",
    "\n",
    "def sentiment_features(begining, body, climax, option1, option2, P1, P2, P3, P4, sentiment_analyzer):\n",
    "    s_begining = sentence_sentiment(begining, sentiment_analyzer)\n",
    "    s_body = sentence_sentiment(body, sentiment_analyzer)\n",
    "    s_climax = sentence_sentiment(climax, sentiment_analyzer)\n",
    "    s_option1 = sentence_sentiment(option1, sentiment_analyzer)\n",
    "    s_option2 = sentence_sentiment(option2, sentiment_analyzer)\n",
    "    s_context = sentence_sentiment(begining+body+climax, sentiment_analyzer)\n",
    "    \n",
    "    features = np.zeros(12)\n",
    "    \n",
    "    features[0] = P1[s_option1, s_begining, s_body, s_climax]\n",
    "    features[1] = P2[s_option1, s_body, s_climax]\n",
    "    features[2] = P3[s_option1, s_climax]\n",
    "    features[3] = P4[s_option1, s_context]\n",
    "    \n",
    "    features[4] = P1[s_option2, s_begining, s_body, s_climax]\n",
    "    features[5] = P2[s_option2, s_body, s_climax]\n",
    "    features[6] = P3[s_option2, s_climax]\n",
    "    features[7] = P4[s_option2, s_context]\n",
    "    \n",
    "    features[8] = 0 if features[0]>features[4] else 1\n",
    "    features[9] = 0 if features[1]>features[5] else 1\n",
    "    features[10] = 0 if features[2]>features[6] else 1\n",
    "    features[11] = 0 if features[3]>features[7] else 1\n",
    "    \n",
    "    return features\n",
    "\n",
    "def generate_vocab(train_stories, vocab_size, vocab_file, corpus_file):\n",
    "    a = time.time()\n",
    "    count = 0\n",
    "    corpus = []\n",
    "    all_tokens = []\n",
    "    for i in tqdm(range(len(train_stories))):\n",
    "        story = train_stories[i]\n",
    "        events = ['<bos>']\n",
    "        for sentence in story:\n",
    "            sentence_pos = nltk.pos_tag(sentence)\n",
    "            for word_pos in sentence_pos:\n",
    "                if 'NN' in word_pos[1] or 'PR' in word_pos[1] or 'VB' in word_pos[1]:\n",
    "                    events.append(word_pos[0])\n",
    "                    all_tokens.append\n",
    "        \n",
    "        corpus.append(events)\n",
    "        all_tokens.extend(events)\n",
    "    \n",
    "    token2count = Counter(all_tokens).most_common(vocab_size)\n",
    "    vocab_tokens = ['<unk>']\n",
    "    vocab_tokens.extend([token for token, count in token2count])\n",
    "    token2id = {token: i for i, token in enumerate(vocab_tokens)}\n",
    "    id2token = {i: token for token, i in token2id.items()}\n",
    "    print(\"Writing Vocabulary Object ... \")\n",
    "    vocab_dict = {\"vocab_size\": vocab_size, \"token2id\": token2id, \"id2token\": id2token}\n",
    "    dump(vocab_file, vocab_dict)\n",
    "    corpus_dict = {\"corpus_size\": len(corpus), \"tokenized_events\": corpus}\n",
    "    dump(corpus_file, corpus_dict)\n",
    "    print(\"Done in {} s\".format(time.time() - a))\n",
    "    \n",
    "    return token2id, id2token, corpus\n",
    "    \n",
    "def dump(write_to_file, dictionary):\n",
    "    out_file = os.path.join(os.getcwd(), write_to_file) # full path\n",
    "    with open(out_file, \"wb+\") as f: # dump object\n",
    "        pickle.dump(dictionary, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "\n",
    "def sentiment_features_train(train_stories):\n",
    "    a = time.time()\n",
    "    P1 = np.zeros((3,3,3,3))\n",
    "    P2 = np.zeros((3,3,3))\n",
    "    P3 = np.zeros((3,3))\n",
    "    P4 = np.zeros((3,3))\n",
    "    sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    for i in tqdm(range(len(train_stories))):\n",
    "        story = train_stories[i]\n",
    "        begining = story[0]\n",
    "        body = story[1] + story[2]\n",
    "        climax = story[3]\n",
    "        ending = story[4]\n",
    "        s_begining = sentence_sentiment(begining, sentiment_analyzer)\n",
    "        s_body = sentence_sentiment(body, sentiment_analyzer)\n",
    "        s_climax = sentence_sentiment(climax, sentiment_analyzer)\n",
    "        s_ending = sentence_sentiment(ending, sentiment_analyzer)\n",
    "        s_context = sentence_sentiment(begining+body+climax, sentiment_analyzer)\n",
    "        \n",
    "        P1[s_ending, s_begining, s_body, s_climax]+=1\n",
    "        P2[s_ending, s_body, s_climax]+=1\n",
    "        P3[s_ending, s_climax]+=1\n",
    "        P4[s_ending, s_context]+=1\n",
    "    \n",
    "    P1 /= np.sum(P1, axis=0)\n",
    "    P2 /= np.sum(P2, axis=0)\n",
    "    P3 /= np.sum(P3, axis=0)\n",
    "    P4 /= np.sum(P4, axis=0)\n",
    "    \n",
    "    print(\"Done in {} s\".format(time.time() - a))\n",
    "    \n",
    "    return P1, P2, P3, P4\n",
    "    \n",
    "\n",
    "def pos_split(sentences):\n",
    "    sentences_pos = nltk.pos_tag(sentences)\n",
    "    sentences_nouns = []\n",
    "    sentences_verbs = []\n",
    "    for word_pos in sentences_pos:\n",
    "        if 'NN' in word_pos[1] or 'PR' in word_pos[1]:\n",
    "            sentences_nouns.append(word_pos[0])\n",
    "        elif 'VB' in word_pos[1]:\n",
    "            sentences_verbs.append(word_pos[0])\n",
    "    \n",
    "    return sentences_nouns, sentences_verbs\n",
    "\n",
    "def extract_event_embeddings(sentences, encoder):\n",
    "    sentences_pos = nltk.pos_tag(sentences)\n",
    "    sentences_event_embeds = []\n",
    "    for word_pos in sentences_pos:\n",
    "        if 'NN' in word_pos[1] or 'PR' in word_pos[1] or 'VB' in word_pos[1]:\n",
    "            sentences_event_embeds.append(encoder[word_pos[0]])\n",
    "    \n",
    "    return sentences_event_embeds\n",
    "\n",
    "def topical_consistency(context, option1, option2, encoder):\n",
    "    context_nouns, context_verbs = pos_split(context)\n",
    "    option1_nouns, option1_verbs = pos_split(option1)\n",
    "    option2_nouns, option2_verbs = pos_split(option2)\n",
    "    \n",
    "    encoded_context_nouns = [encoder[context_noun] for context_noun in context_nouns]\n",
    "    encoded_option1_nouns = [encoder[option1_noun] for option1_noun in option1_nouns]\n",
    "    encoded_option2_nouns = [encoder[option2_noun] for option2_noun in option2_nouns]\n",
    "    encoded_context_verbs = [encoder[context_verb] for context_verb in context_verbs]\n",
    "    encoded_option1_verbs = [encoder[option1_verb] for option1_verb in option1_verbs]\n",
    "    encoded_option2_verbs = [encoder[option2_verb] for option2_verb in option2_verbs]\n",
    "    \n",
    "    option1_nouns_similarity_matrix = cosine_similarity(encoded_option1_nouns, encoded_context_nouns)\n",
    "    option2_nouns_similarity_matrix = cosine_similarity(encoded_option2_nouns, encoded_context_nouns)\n",
    "    option1_verbs_similarity_matrix = cosine_similarity(encoded_option1_verbs, encoded_context_verbs)\n",
    "    option2_verbs_similarity_matrix = cosine_similarity(encoded_option2_verbs, encoded_context_verbs)\n",
    "    \n",
    "    option1_nouns_similarity = np.max(option1_nouns_similarity_matrix, axis=1)\n",
    "    option2_nouns_similarity = np.max(option2_nouns_similarity_matrix, axis=1)\n",
    "    option1_verbs_similarity = np.max(option1_verbs_similarity_matrix, axis=1)\n",
    "    option2_verbs_similarity = np.max(option2_verbs_similarity_matrix, axis=1)\n",
    "    \n",
    "    option1_score = np.mean(np.concatenate(option1_nouns_similarity,option1_verbs_similarity))\n",
    "    option2_score = np.mean(np.concatenate(option2_nouns_similarity,option2_verbs_similarity))\n",
    "    \n",
    "    comparative = 0 if option1_score>option2_score else 1\n",
    "    \n",
    "    return option1_score, option2_score, comparative\n",
    "\n",
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading stories to memory ...\n",
      "Done in 797.9813842773438 s\n"
     ]
    }
   ],
   "source": [
    "train_stories = get_stories_as_lists(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/88161 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 4/88161 [00:00<43:56, 33.43it/s]\u001b[A\n",
      "  0%|          | 8/88161 [00:00<44:01, 33.38it/s]\u001b[A\n",
      "  0%|          | 12/88161 [00:00<44:21, 33.12it/s]\u001b[A\n",
      "  0%|          | 15/88161 [00:00<45:30, 32.28it/s]\u001b[A\n",
      "  0%|          | 19/88161 [00:00<44:27, 33.05it/s]\u001b[A\n",
      "  0%|          | 22/88161 [00:00<45:33, 32.24it/s]\u001b[A\n",
      "  0%|          | 26/88161 [00:00<45:31, 32.27it/s]\u001b[A\n",
      "  0%|          | 30/88161 [00:00<45:02, 32.61it/s]\u001b[A\n",
      "  0%|          | 34/88161 [00:01<44:59, 32.64it/s]\u001b[A\n",
      "  0%|          | 38/88161 [00:01<44:23, 33.08it/s]\u001b[A\n",
      "  0%|          | 42/88161 [00:01<44:28, 33.03it/s]\u001b[A\n",
      "  0%|          | 46/88161 [00:01<43:41, 33.61it/s]\u001b[A\n",
      "  0%|          | 50/88161 [00:01<43:13, 33.97it/s]\u001b[A\n",
      "  0%|          | 54/88161 [00:01<43:02, 34.12it/s]\u001b[A\n",
      "  0%|          | 58/88161 [00:01<43:22, 33.86it/s]\u001b[A\n",
      "  0%|          | 62/88161 [00:01<43:15, 33.94it/s]\u001b[A\n",
      "  0%|          | 66/88161 [00:01<43:43, 33.58it/s]\u001b[A\n",
      "  0%|          | 70/88161 [00:02<43:41, 33.60it/s]\u001b[A\n",
      "  0%|          | 75/88161 [00:02<43:07, 34.05it/s]\u001b[A\n",
      "  0%|          | 79/88161 [00:02<43:35, 33.68it/s]\u001b[A\n",
      "  0%|          | 83/88161 [00:02<43:42, 33.58it/s]\u001b[A\n",
      "  0%|          | 87/88161 [00:02<43:28, 33.77it/s]\u001b[A\n",
      "  0%|          | 91/88161 [00:02<43:21, 33.85it/s]\u001b[A\n",
      "  0%|          | 95/88161 [00:02<43:15, 33.93it/s]\u001b[A\n",
      "  0%|          | 99/88161 [00:02<43:25, 33.79it/s]\u001b[A\n",
      "  0%|          | 103/88161 [00:03<43:19, 33.87it/s]\u001b[A\n",
      "  0%|          | 107/88161 [00:03<43:17, 33.90it/s]\u001b[A\n",
      "  0%|          | 111/88161 [00:03<43:26, 33.79it/s]\u001b[A\n",
      "  0%|          | 115/88161 [00:03<43:31, 33.71it/s]\u001b[A\n",
      "  0%|          | 119/88161 [00:03<43:37, 33.64it/s]\u001b[A\n",
      "  0%|          | 123/88161 [00:03<43:23, 33.82it/s]\u001b[A\n",
      "  0%|          | 127/88161 [00:03<43:31, 33.71it/s]\u001b[A\n",
      "  0%|          | 131/88161 [00:03<43:32, 33.70it/s]\u001b[A\n",
      "  0%|          | 135/88161 [00:04<43:34, 33.67it/s]\u001b[A\n",
      "  0%|          | 139/88161 [00:04<43:24, 33.80it/s]\u001b[A\n",
      "  0%|          | 143/88161 [00:04<43:32, 33.69it/s]\u001b[A\n",
      "  0%|          | 147/88161 [00:04<43:40, 33.58it/s]\u001b[A\n",
      "  0%|          | 151/88161 [00:04<43:49, 33.46it/s]\u001b[A\n",
      "  0%|          | 164/88161 [00:04<44:03, 33.28it/s]Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/srikanth_sarma/.conda/envs/tensorflow/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/srikanth_sarma/.local/lib/python3.5/site-packages/tqdm/_monitor.py\", line 62, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/home/srikanth_sarma/.conda/envs/tensorflow/lib/python3.5/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n",
      "100%|██████████| 88161/88161 [40:18<00:00, 36.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 2418.6640543937683 s\n"
     ]
    }
   ],
   "source": [
    "P1, P2, P3, P4 = sentiment_features_train(train_stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_file = open('sentement_params.pkl', 'wb')\n",
    "\n",
    "data = {'P1':P1, 'P2': P2, 'P3': P3, 'P4': P4}\n",
    "\n",
    "pickle.dump(data, pkl_file)\n",
    "pkl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P4[0,1] + P4[1,1] + P4[2,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "gloveFile = './glove.6B.50d.txt'\n",
    "encoder = loadGloveModel(gloveFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>InputSentence1</th>\n",
       "      <th>InputSentence2</th>\n",
       "      <th>InputSentence3</th>\n",
       "      <th>InputSentence4</th>\n",
       "      <th>RandomFifthSentenceQuiz1</th>\n",
       "      <th>RandomFifthSentenceQuiz2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rick grew up in a troubled household.</td>\n",
       "      <td>He never found good support in family, and tur...</td>\n",
       "      <td>It wasn't long before Rick got shot in a robbery.</td>\n",
       "      <td>The incident caused him to turn a new leaf.</td>\n",
       "      <td>He is happy now.</td>\n",
       "      <td>He joined a gang.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Laverne needs to prepare something for her fri...</td>\n",
       "      <td>She decides to bake a batch of brownies.</td>\n",
       "      <td>She chooses a recipe and follows it closely.</td>\n",
       "      <td>Laverne tests one of the brownies to make sure...</td>\n",
       "      <td>The brownies are so delicious Laverne eats two...</td>\n",
       "      <td>Laverne doesn't go to her friend's party.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sarah had been dreaming of visiting Europe for...</td>\n",
       "      <td>She had finally saved enough for the trip.</td>\n",
       "      <td>She landed in Spain and traveled east across t...</td>\n",
       "      <td>She didn't like how different everything was.</td>\n",
       "      <td>Sarah then decided to move to Europe.</td>\n",
       "      <td>Sarah decided that she preferred her home over...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gina was worried the cookie dough in the tube ...</td>\n",
       "      <td>She was very happy to find she was wrong.</td>\n",
       "      <td>The cookies from the tube were as good as from...</td>\n",
       "      <td>Gina intended to only eat 2 cookies and save t...</td>\n",
       "      <td>Gina liked the cookies so much she ate them al...</td>\n",
       "      <td>Gina gave the cookies away at her church.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It was  my final performance in marching band.</td>\n",
       "      <td>I was playing the snare drum in the band.</td>\n",
       "      <td>We played Thriller and Radar Love.</td>\n",
       "      <td>The performance was flawless.</td>\n",
       "      <td>I was very proud of my performance.</td>\n",
       "      <td>I was very ashamed of my performance.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      InputSentence1  \\\n",
       "0              Rick grew up in a troubled household.   \n",
       "1  Laverne needs to prepare something for her fri...   \n",
       "2  Sarah had been dreaming of visiting Europe for...   \n",
       "3  Gina was worried the cookie dough in the tube ...   \n",
       "4     It was  my final performance in marching band.   \n",
       "\n",
       "                                      InputSentence2  \\\n",
       "0  He never found good support in family, and tur...   \n",
       "1           She decides to bake a batch of brownies.   \n",
       "2         She had finally saved enough for the trip.   \n",
       "3          She was very happy to find she was wrong.   \n",
       "4          I was playing the snare drum in the band.   \n",
       "\n",
       "                                      InputSentence3  \\\n",
       "0  It wasn't long before Rick got shot in a robbery.   \n",
       "1       She chooses a recipe and follows it closely.   \n",
       "2  She landed in Spain and traveled east across t...   \n",
       "3  The cookies from the tube were as good as from...   \n",
       "4                 We played Thriller and Radar Love.   \n",
       "\n",
       "                                      InputSentence4  \\\n",
       "0        The incident caused him to turn a new leaf.   \n",
       "1  Laverne tests one of the brownies to make sure...   \n",
       "2      She didn't like how different everything was.   \n",
       "3  Gina intended to only eat 2 cookies and save t...   \n",
       "4                      The performance was flawless.   \n",
       "\n",
       "                            RandomFifthSentenceQuiz1  \\\n",
       "0                                   He is happy now.   \n",
       "1  The brownies are so delicious Laverne eats two...   \n",
       "2              Sarah then decided to move to Europe.   \n",
       "3  Gina liked the cookies so much she ate them al...   \n",
       "4                I was very proud of my performance.   \n",
       "\n",
       "                            RandomFifthSentenceQuiz2  \n",
       "0                                  He joined a gang.  \n",
       "1          Laverne doesn't go to her friend's party.  \n",
       "2  Sarah decided that she preferred her home over...  \n",
       "3          Gina gave the cookies away at her church.  \n",
       "4              I was very ashamed of my performance.  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading stories to memory ...\n",
      "Done in 22.334588289260864 s\n"
     ]
    }
   ],
   "source": [
    "valid_stories = get_stories_as_lists(val_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1871/1871 [00:54<00:00, 34.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 54.696404695510864 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "a = time.time()\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "n = len(valid_stories)\n",
    "features = np.zeros((n, 12))\n",
    "y = np.zeros(n, dtype = int)\n",
    "for i in tqdm(range(n)):\n",
    "    story = valid_stories[i]\n",
    "    begining = story[0]\n",
    "    body = story[1] + story[2]\n",
    "    climax = story[3]\n",
    "    option1 = story[4]\n",
    "    option2 = story[5]\n",
    "    \n",
    "    sf = sentiment_features(begining, body, climax, option1, option2, P1, P2, P3, P4, sentiment_analyzer)\n",
    "    \n",
    "    context = begining + body + climax\n",
    "    \n",
    "#     tf = topical_consistency(context, option1, option2, encoder)\n",
    "    \n",
    "#     features[i] = np.concatenate((sf, tf))\n",
    "    features[i] = sf\n",
    "    \n",
    "    y[i] = val_answer[i]-1\n",
    "\n",
    "print(\"Done in {} s\".format(time.time() - a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionClass():\n",
    "    def __init__(self, learning_rate=0.01, stopping_criterion=0.01, max_epochs=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.stopping_criterion = stopping_criterion\n",
    "        self.max_epochs = max_epochs\n",
    "        self.w = None\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        m = x.shape[0]\n",
    "        n = x.shape[1]\n",
    "        self.w = np.random.randn(n)\n",
    "        gradient = np.zeros(n)\n",
    "        cost = np.zeros(self.max_epochs)\n",
    "        \n",
    "        for epoch in tqdm(range(0,self.max_epochs)):\n",
    "            for i in range(0,m):\n",
    "                y_hat=expit(np.dot(self.w,x[i,:]))\n",
    "                gradient=(y_hat-y[i])*x[i,:]\n",
    "                self.w-=self.learning_rate*gradient\n",
    "                cost[epoch]-=(y[i]*np.log(y_hat)+(1-y[i])*np.log(1-y_hat))\n",
    "        \n",
    "        plt.plot(cost)\n",
    "        return self.w\n",
    "    \n",
    "    def predict(self, x):\n",
    "        m = x.shape[0]\n",
    "        n = x.shape[1]\n",
    "        y = np.zeros(m)\n",
    "        for i in range(0,m):\n",
    "            y[i] = 1 if expit(np.dot(self.w,x[i,:]))<0.5 else 2\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [06:45<00:00,  2.47it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efb88f04cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = model.fit(features, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5847140566541956\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(features)\n",
    "\n",
    "print(accuracy_score(y+1, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading stories to memory ...\n",
      "Done in 21.386431217193604 s\n"
     ]
    }
   ],
   "source": [
    "test_stories = get_stories_as_lists(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1871/1871 [00:53<00:00, 34.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 54.02568602561951 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "a = time.time()\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "n = len(test_stories)\n",
    "test_features = np.zeros((n, 12))\n",
    "y_gt = np.zeros(n, dtype = int)\n",
    "for i in tqdm(range(n)):\n",
    "    story = test_stories[i]\n",
    "    begining = story[0]\n",
    "    body = story[1] + story[2]\n",
    "    climax = story[3]\n",
    "    option1 = story[4]\n",
    "    option2 = story[5]\n",
    "    \n",
    "    sf = sentiment_features(begining, body, climax, option1, option2, P1, P2, P3, P4, sentiment_analyzer)\n",
    "    \n",
    "    context = begining + body + climax\n",
    "    \n",
    "#     tf = topical_consistency(context, option1, option2, encoder)\n",
    "    \n",
    "#     features[i] = np.concatenate((sf, tf))\n",
    "    test_features[i] = sf\n",
    "    \n",
    "    y_gt[i] = test_answer[i]\n",
    "\n",
    "print(\"Done in {} s\".format(time.time() - a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5713522180652058\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(test_features)\n",
    "\n",
    "print(accuracy_score(y_gt, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.52086746, 1.42205282, 1.74531345, 1.57993736, 1.434772  ,\n",
       "       1.46124831, 1.38297441, 1.5098553 , 1.39829538, 1.33655118])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44901828"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88161/88161 [40:38<00:00, 36.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Vocabulary Object ... \n",
      "Done in 2443.131096124649 s\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 20000\n",
    "vocab_file = './vocab.pkl'\n",
    "corpus_file = './corpus.pkl'\n",
    "token2id, id2token, corpus = generate_vocab(train_stories, vocab_size, vocab_file, corpus_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, hidden_size=512, embedding_size=100, vocab_size=20000):\n",
    "        self.dtype = tf.float32\n",
    "        initializer = tf.contrib.layers.xavier_initializer(dtype=self.dtype)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.W = tf.Variable(initializer((hidden_size, vocab_size)), name=\"W\", dtype=self.dtype)\n",
    "        self.biases = tf.Variable(initializer([vocab_size]), name=\"biases\", dtype=self.dtype)\n",
    "        \n",
    "        self.embedding_matrix = tf.Variable(initializer((vocab_size, embedding_size)), name=\"embedding\",\n",
    "                                                dtype=self.dtype)\n",
    "        self.rnn = Cell(num_units=hidden_size, initializer=initializer, name=\"cell\")\n",
    "        self.c_in = tf.placeholder(tf.float32, shape=[1, self.rnn.state_size.c], name='c_in')\n",
    "        self.h_in = tf.placeholder(tf.float32, shape=[1, self.rnn.state_size.h], name='h_in')\n",
    "\n",
    "        self.int_state = tf.contrib.rnn.LSTMStateTuple(self.c_in, self.h_in)  # internal state\n",
    "        self.current_word = tf.placeholder(tf.int32, [1])\n",
    "        self.next_word = tf.placeholder(tf.int32, [1])\n",
    "        self.current_embed = tf.nn.embedding_lookup(self.embedding_matrix, self.current_word)\n",
    "\n",
    "        next_output, self.next_state = self.rnn(self.current_embed, self.int_state)\n",
    "        next_logits = tf.matmul(next_output, self.W) + self.biases\n",
    "        self.next_probabs = tf.nn.softmax(next_logits)\n",
    "        \n",
    "        self.loss = self.compute_loss(logits=next_logits, labels=self.next_word)\n",
    "        self.optimizer = tf.train.AdamOptimizer()\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.train_op = self.get_train_op()\n",
    "        \n",
    "    def compute_loss(self, logits, labels):\n",
    "        self.losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "        return tf.reduce_sum(self.losses)\n",
    "    \n",
    "    def get_train_op(self):\n",
    "        return self.optimizer.minimize(self.loss)\n",
    "    \n",
    "    def save_model(self, sess, path):\n",
    "        self.saver.save(sess, path)\n",
    "        print(\"Model saved at %s\" % path)\n",
    "\n",
    "    def load_model(self, sess, path):\n",
    "        try:  # try to load the model\n",
    "            self.saver.restore(sess, path)\n",
    "            print(\"Model restored from %s\" % path)\n",
    "            return True\n",
    "        except:\n",
    "            print(\"Couldn't restore model\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 30/176322 [00:07<11:57:22,  4.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 1, loss 287.10278129577637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 58/176322 [00:13<11:12:57,  4.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 2, loss 287.1250858306885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 104/176322 [00:23<10:12:44,  4.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 3, loss 454.03917503356934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 129/176322 [00:29<10:46:28,  4.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 4, loss 235.7351942062378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 160/176322 [00:35<9:56:41,  4.92it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 5, loss 301.3549575805664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 175/176322 [00:39<10:54:41,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 6, loss 146.93242263793945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 199/176322 [00:44<11:25:59,  4.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 7, loss 205.07554388046265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 211/176322 [00:47<11:21:17,  4.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word crushes doesn't exist in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 212/176322 [00:47<11:15:29,  4.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word crushes doesn't exist in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 229/176322 [00:51<10:40:03,  4.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 8, loss 245.32557773590088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 257/176322 [00:57<10:41:37,  4.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 9, loss 253.61032247543335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 282/176322 [01:03<9:53:28,  4.94it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 10, loss 189.2352638244629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 308/176322 [01:08<9:35:51,  5.09it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 11, loss 230.24086141586304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 310/176322 [01:08<10:06:16,  4.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word bioluminescence doesn't exist in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 312/176322 [01:09<10:26:41,  4.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word bioluminescence doesn't exist in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 313/176322 [01:09<10:33:16,  4.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word dinoflagellates doesn't exist in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 315/176322 [01:09<10:28:53,  4.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word dinoflagellates doesn't exist in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 318/176322 [01:10<10:19:39,  4.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word kayaked doesn't exist in the vocabulary\n",
      "The word kayaked doesn't exist in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 327/176322 [01:12<10:09:06,  4.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 12, loss 171.81093454360962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 350/176322 [01:17<10:06:30,  4.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 13, loss 185.59898924827576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 383/176322 [01:24<9:45:26,  5.01it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 14, loss 292.92524909973145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 420/176322 [01:32<9:34:39,  5.10it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 15, loss 285.16637659072876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 446/176322 [01:37<9:53:13,  4.94it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 16, loss 227.62650513648987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 467/176322 [01:42<10:16:38,  4.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 17, loss 172.42100477218628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 496/176322 [01:48<9:44:22,  5.01it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 18, loss 228.10159373283386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 521/176322 [01:53<10:08:07,  4.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 19, loss 207.34536981582642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 547/176322 [01:58<10:26:18,  4.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 20, loss 255.28839445114136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 572/176322 [02:03<10:07:07,  4.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 21, loss 194.60378694534302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 595/176322 [02:08<9:57:11,  4.90it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 22, loss 174.35734057426453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 615/176322 [02:12<9:54:21,  4.93it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 23, loss 189.97795844078064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 635/176322 [02:16<9:32:51,  5.11it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 24, loss 128.85306978225708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 671/176322 [02:24<10:00:38,  4.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 25, loss 273.1967725753784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 690/176322 [02:28<9:30:43,  5.13it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 26, loss 148.8140206336975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 717/176322 [02:33<10:18:39,  4.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 27, loss 248.40619659423828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 724/176322 [02:35<10:55:56,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word feign doesn't exist in the vocabulary\n",
      "The word feign doesn't exist in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 739/176322 [02:38<10:07:46,  4.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word depicting doesn't exist in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 740/176322 [02:38<10:09:34,  4.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word depicting doesn't exist in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 741/176322 [02:38<10:38:35,  4.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word greatness doesn't exist in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 742/176322 [02:39<10:34:56,  4.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word greatness doesn't exist in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 746/176322 [02:39<10:28:23,  4.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 28, loss 232.50163531303406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 773/176322 [02:45<9:54:22,  4.92it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 29, loss 192.9961552619934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 798/176322 [02:50<9:45:51,  4.99it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 30, loss 206.80044603347778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 814/176322 [02:54<9:44:53,  5.00it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 31, loss 126.85681962966919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 840/176322 [02:59<9:54:42,  4.92it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 32, loss 222.82199144363403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 875/176322 [03:06<10:21:58,  4.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 33, loss 257.3995244503021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 907/176322 [03:13<11:50:16,  4.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 34, loss 277.73566603660583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 930/176322 [03:19<12:09:35,  4.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 35, loss 196.04649353027344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 951/176322 [03:23<9:14:53,  5.27it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 36, loss 167.43369102478027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 982/176322 [03:29<8:48:03,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 37, loss 251.99108242988586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1015/176322 [03:35<8:41:04,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 38, loss 238.26146268844604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1042/176322 [03:39<8:49:54,  5.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 39, loss 237.4956874847412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1044/176322 [03:40<9:23:01,  5.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word kpop doesn't exist in the vocabulary\n",
      "The word kpop doesn't exist in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1068/176322 [03:44<8:38:03,  5.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 40, loss 215.74927473068237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1097/176322 [03:49<8:43:01,  5.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 41, loss 243.0179009437561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1123/176322 [03:54<8:39:13,  5.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 42, loss 235.85043907165527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1135/176322 [03:56<8:40:10,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word bar-b-que doesn't exist in the vocabulary\n",
      "The word bar-b-que doesn't exist in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1155/176322 [04:00<8:42:47,  5.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 43, loss 285.29610681533813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1178/176322 [04:04<8:42:16,  5.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 44, loss 193.6609172821045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1211/176322 [04:10<8:39:49,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word SR doesn't exist in the vocabulary\n",
      "The word SR doesn't exist in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1213/176322 [04:10<8:52:13,  5.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 45, loss 281.1748044490814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1252/176322 [04:17<8:37:04,  5.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 46, loss 320.0289936065674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1282/176322 [04:23<8:48:08,  5.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 47, loss 231.096919298172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1306/176322 [04:27<8:43:46,  5.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 48, loss 195.42239499092102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1322/176322 [04:30<8:40:43,  5.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word tweaking doesn't exist in the vocabulary\n",
      "The word tweaking doesn't exist in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1325/176322 [04:30<8:54:09,  5.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word soundboard doesn't exist in the vocabulary\n",
      "The word soundboard doesn't exist in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1329/176322 [04:31<8:57:47,  5.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 49, loss 180.86245679855347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1362/176322 [04:37<8:42:43,  5.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 50, loss 253.33163142204285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1389/176322 [04:42<8:38:48,  5.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 51, loss 196.7431125640869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1415/176322 [04:47<8:47:38,  5.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 52, loss 212.56882905960083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1441/176322 [04:51<8:59:41,  5.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 53, loss 200.95927000045776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1458/176322 [04:54<8:42:47,  5.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word Wondering doesn't exist in the vocabulary\n",
      "The word Wondering doesn't exist in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1472/176322 [04:57<8:40:17,  5.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 54, loss 245.52580547332764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1500/176322 [05:02<8:39:40,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 55, loss 244.6576910018921\n",
      "The word Wando doesn't exist in the vocabulary\n",
      "The word Wando doesn't exist in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1530/176322 [05:07<8:40:32,  5.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 56, loss 238.3613829612732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1552/176322 [05:11<8:37:31,  5.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word refining doesn't exist in the vocabulary\n",
      "The word refining doesn't exist in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1558/176322 [05:12<8:49:19,  5.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word undertaking doesn't exist in the vocabulary\n",
      "epoch 1, story 57, loss 236.01052141189575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1577/176322 [05:16<8:49:47,  5.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 58, loss 143.78891706466675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1606/176322 [05:21<8:40:42,  5.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 59, loss 240.5543031692505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1627/176322 [05:25<8:46:55,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 60, loss 175.9181170463562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1629/176322 [05:25<8:55:14,  5.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word ledger doesn't exist in the vocabulary\n",
      "The word ledger doesn't exist in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1651/176322 [05:29<8:38:01,  5.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word searchers doesn't exist in the vocabulary\n",
      "The word searchers doesn't exist in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1656/176322 [05:30<8:48:05,  5.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 61, loss 251.34808111190796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1687/176322 [05:36<8:44:42,  5.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 62, loss 261.4562644958496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1706/176322 [05:39<9:02:34,  5.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 63, loss 172.17145490646362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1736/176322 [05:45<8:50:56,  5.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 64, loss 236.02072429656982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1764/176322 [05:50<8:45:55,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 65, loss 215.33836317062378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1796/176322 [05:55<8:42:36,  5.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 66, loss 227.67550325393677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1819/176322 [06:00<8:47:46,  5.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 67, loss 169.95350980758667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1847/176322 [06:05<8:48:40,  5.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 68, loss 223.32368898391724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1864/176322 [06:08<8:47:30,  5.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word Toothpaste doesn't exist in the vocabulary\n",
      "The word Toothpaste doesn't exist in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1875/176322 [06:10<8:47:59,  5.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 69, loss 207.7210624217987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1908/176322 [06:16<8:43:36,  5.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 70, loss 268.81965923309326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1935/176322 [06:20<8:48:42,  5.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 71, loss 182.98750591278076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1967/176322 [06:26<8:43:56,  5.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 72, loss 255.23822236061096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1998/176322 [06:32<9:01:47,  5.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word barf doesn't exist in the vocabulary\n",
      "The word barf doesn't exist in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2000/176322 [06:32<9:04:36,  5.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 73, loss 254.77456951141357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2031/176322 [06:38<8:41:41,  5.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 74, loss 233.46911644935608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2053/176322 [06:42<8:42:55,  5.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 75, loss 177.70243644714355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2076/176322 [06:46<8:41:48,  5.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 76, loss 170.92146348953247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2098/176322 [06:50<8:41:15,  5.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 77, loss 192.91416120529175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2124/176322 [06:55<8:43:41,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 78, loss 205.98660802841187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2153/176322 [07:00<8:38:50,  5.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 79, loss 244.5765142440796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2181/176322 [07:05<8:42:37,  5.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 80, loss 253.76702785491943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 2208/176322 [07:10<8:44:28,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 81, loss 209.05631685256958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 2230/176322 [07:14<8:40:08,  5.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 82, loss 159.90438723564148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 2258/176322 [07:19<8:53:48,  5.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 83, loss 247.21933269500732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 2279/176322 [07:23<8:43:42,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 84, loss 148.68370294570923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 2305/176322 [07:27<8:41:33,  5.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 85, loss 227.40154933929443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 2337/176322 [07:33<8:37:59,  5.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 86, loss 243.71235466003418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 2360/176322 [07:37<8:45:36,  5.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 87, loss 188.66959238052368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 2382/176322 [07:41<8:35:03,  5.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 88, loss 180.69088554382324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 2401/176322 [07:45<8:40:14,  5.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 89, loss 149.9928548336029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 2426/176322 [07:49<8:40:07,  5.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 90, loss 166.8393750190735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 2451/176322 [07:54<8:40:21,  5.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 91, loss 180.72937488555908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 2471/176322 [07:57<8:36:33,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 92, loss 138.45806694030762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 2499/176322 [08:03<8:46:44,  5.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 93, loss 216.85514783859253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 2526/176322 [08:07<8:40:46,  5.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 94, loss 193.01974439620972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 2548/176322 [08:11<8:39:06,  5.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 95, loss 192.70817816257477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 2573/176322 [08:16<8:50:00,  5.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 96, loss 190.36271905899048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 2594/176322 [08:20<8:40:50,  5.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 97, loss 186.01100873947144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 2611/176322 [08:23<8:38:19,  5.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word icewater doesn't exist in the vocabulary\n",
      "The word icewater doesn't exist in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 2626/176322 [08:25<8:41:06,  5.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 98, loss 268.88182735443115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2652/176322 [08:30<8:45:46,  5.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 99, loss 194.79986143112183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2684/176322 [08:36<8:38:13,  5.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 100, loss 224.82326912879944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2708/176322 [08:40<8:33:53,  5.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 101, loss 175.55713820457458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2731/176322 [08:45<8:48:31,  5.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 102, loss 182.1000428199768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2758/176322 [08:49<8:41:14,  5.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 103, loss 230.0084090232849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2788/176322 [08:55<8:39:35,  5.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 104, loss 223.09881258010864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2821/176322 [09:01<8:39:56,  5.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 105, loss 226.626971244812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2846/176322 [09:05<8:37:33,  5.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 106, loss 214.33619952201843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2868/176322 [09:09<8:37:02,  5.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 107, loss 157.77446508407593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2889/176322 [09:13<8:41:06,  5.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 108, loss 168.87482166290283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2913/176322 [09:17<8:29:00,  5.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 109, loss 176.59085083007812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2936/176322 [09:22<8:41:54,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word Valium doesn't exist in the vocabulary\n",
      "The word Valium doesn't exist in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2941/176322 [09:22<8:42:59,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 110, loss 207.79543471336365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2964/176322 [09:27<8:45:00,  5.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 111, loss 169.47826600074768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2987/176322 [09:31<8:45:31,  5.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 112, loss 181.75489115715027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3011/176322 [09:35<8:34:57,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 113, loss 164.2197139263153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3026/176322 [09:38<8:35:37,  5.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 114, loss 109.84715366363525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3054/176322 [09:43<8:41:34,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word roundtrip doesn't exist in the vocabulary\n",
      "The word roundtrip doesn't exist in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3064/176322 [09:45<8:45:39,  5.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 115, loss 297.0855185985565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3071/176322 [09:46<8:40:00,  5.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word centennial doesn't exist in the vocabulary\n",
      "The word centennial doesn't exist in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3090/176322 [09:49<8:41:59,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 116, loss 215.12577199935913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3113/176322 [09:54<8:46:31,  5.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 117, loss 172.25374913215637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3127/176322 [09:56<8:36:59,  5.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 118, loss 86.55316114425659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3151/176322 [10:00<8:33:50,  5.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 119, loss 157.8844702243805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3179/176322 [10:05<8:41:06,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 120, loss 237.18412017822266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3204/176322 [10:10<8:37:24,  5.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word GET doesn't exist in the vocabulary\n",
      "The word GET doesn't exist in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3207/176322 [10:11<8:53:49,  5.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word SUPPOSEDLY doesn't exist in the vocabulary\n",
      "The word SUPPOSEDLY doesn't exist in the vocabulary\n",
      "The word HAPPENED doesn't exist in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3209/176322 [10:11<8:50:49,  5.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word HAPPENED doesn't exist in the vocabulary\n",
      "The word YEARS doesn't exist in the vocabulary\n",
      "The word YEARS doesn't exist in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3217/176322 [10:12<8:40:50,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 121, loss 286.5079231262207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3242/176322 [10:17<8:41:52,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 122, loss 193.1323127746582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3261/176322 [10:21<8:44:58,  5.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word Orchid doesn't exist in the vocabulary\n",
      "The word Orchid doesn't exist in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3277/176322 [10:24<8:45:07,  5.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 123, loss 280.9358010292053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3285/176322 [10:25<8:40:05,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word bughouse doesn't exist in the vocabulary\n",
      "The word bughouse doesn't exist in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3305/176322 [10:29<8:42:37,  5.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 124, loss 226.33205890655518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3336/176322 [10:34<8:36:53,  5.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 125, loss 227.21790266036987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3363/176322 [10:39<8:35:51,  5.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 126, loss 221.9933466911316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3392/176322 [10:44<8:33:53,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 127, loss 223.8738169670105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3428/176322 [10:51<8:45:35,  5.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 128, loss 297.56982707977295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3449/176322 [10:55<8:39:48,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 129, loss 191.51001739501953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3471/176322 [10:59<8:39:03,  5.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 130, loss 150.09319591522217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3474/176322 [10:59<8:50:21,  5.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word scoreless doesn't exist in the vocabulary\n",
      "The word scoreless doesn't exist in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3481/176322 [11:00<8:46:58,  5.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word outfielders doesn't exist in the vocabulary\n",
      "The word outfielders doesn't exist in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3491/176322 [11:02<8:39:52,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word homer doesn't exist in the vocabulary\n",
      "The word homer doesn't exist in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3497/176322 [11:03<8:40:33,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 131, loss 208.54867935180664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3519/176322 [11:07<8:39:05,  5.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 132, loss 155.21917247772217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3538/176322 [11:11<8:42:31,  5.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 133, loss 130.57731866836548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3557/176322 [11:14<8:32:57,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 134, loss 128.24663877487183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3585/176322 [11:19<8:37:51,  5.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 135, loss 205.86271810531616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3605/176322 [11:23<8:39:45,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 136, loss 177.62080669403076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3630/176322 [11:27<8:39:15,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 137, loss 197.7232723236084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3657/176322 [11:32<8:32:20,  5.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 138, loss 215.77495074272156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3685/176322 [11:37<8:35:08,  5.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 139, loss 211.83275198936462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3716/176322 [11:43<8:37:01,  5.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 140, loss 228.24469590187073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3753/176322 [11:50<8:40:21,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 141, loss 303.70889496803284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3779/176322 [11:54<8:36:14,  5.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 142, loss 183.002051115036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3801/176322 [11:58<8:41:07,  5.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 143, loss 160.9749653339386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3831/176322 [12:04<8:40:27,  5.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 144, loss 242.6812620162964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3845/176322 [12:06<8:37:42,  5.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word gunk doesn't exist in the vocabulary\n",
      "The word gunk doesn't exist in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3852/176322 [12:08<8:49:16,  5.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 145, loss 156.18124675750732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3882/176322 [12:13<8:39:53,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 146, loss 223.71704626083374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3916/176322 [12:19<8:36:00,  5.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 147, loss 240.4567539691925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3941/176322 [12:24<8:43:18,  5.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word differents doesn't exist in the vocabulary\n",
      "The word differents doesn't exist in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3944/176322 [12:24<8:52:36,  5.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 148, loss 206.5506467819214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3972/176322 [12:29<8:35:31,  5.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 149, loss 193.24309277534485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3999/176322 [12:34<8:37:12,  5.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 150, loss 225.709632396698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4030/176322 [12:40<8:36:04,  5.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 151, loss 211.79109954833984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4060/176322 [12:45<8:35:11,  5.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 152, loss 220.78247833251953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4079/176322 [12:49<8:38:22,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word Ticks doesn't exist in the vocabulary\n",
      "The word Ticks doesn't exist in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4083/176322 [12:49<8:50:16,  5.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 153, loss 190.62562441825867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4110/176322 [12:54<8:36:16,  5.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 154, loss 208.7792248725891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4113/176322 [12:55<8:52:07,  5.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word Annual doesn't exist in the vocabulary\n",
      "The word Annual doesn't exist in the vocabulary\n",
      "The word Cooking doesn't exist in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4115/176322 [12:55<8:51:31,  5.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word Cooking doesn't exist in the vocabulary\n",
      "The word Contest doesn't exist in the vocabulary\n",
      "The word Contest doesn't exist in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4142/176322 [13:00<8:32:28,  5.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 155, loss 244.29165744781494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4172/176322 [13:05<8:31:45,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 156, loss 251.55711340904236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4201/176322 [13:11<8:40:44,  5.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 157, loss 222.6339316368103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4224/176322 [13:15<8:38:54,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 158, loss 189.89273071289062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4247/176322 [13:19<8:39:19,  5.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 159, loss 169.42269468307495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4268/176322 [13:23<8:46:47,  5.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 160, loss 198.07212591171265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4289/176322 [13:27<8:37:44,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word re-order doesn't exist in the vocabulary\n",
      "The word re-order doesn't exist in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4291/176322 [13:27<8:45:32,  5.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 161, loss 164.87524938583374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4320/176322 [13:32<8:33:51,  5.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 162, loss 190.3411991596222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4352/176322 [13:38<8:30:12,  5.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 163, loss 222.0591378211975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4378/176322 [13:43<8:39:50,  5.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 164, loss 222.13504838943481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 4416/176322 [13:49<8:30:36,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 165, loss 297.93074774742126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 4444/176322 [13:54<8:33:07,  5.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 166, loss 214.88024401664734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 4477/176322 [14:00<8:35:36,  5.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 167, loss 246.58170318603516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 4490/176322 [14:03<8:36:21,  5.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word garb doesn't exist in the vocabulary\n",
      "The word garb doesn't exist in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 4508/176322 [14:06<8:32:57,  5.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 168, loss 263.265145778656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 4534/176322 [14:11<9:11:27,  5.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 169, loss 184.14086961746216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 4559/176322 [14:16<8:53:43,  5.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 170, loss 174.3669936656952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 4567/176322 [14:17<9:01:18,  5.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word equipped doesn't exist in the vocabulary\n",
      "The word equipped doesn't exist in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 4583/176322 [14:20<9:19:52,  5.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 171, loss 221.29760694503784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 4607/176322 [14:25<9:44:06,  4.90it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, story 172, loss 176.59210467338562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 4622/176322 [14:28<9:09:09,  5.21it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-4aa46dbcef01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m                     feed_dict = {model.current_word: [ind], model.c_in: c_init, model.h_in: h_init,\n\u001b[1;32m     30\u001b[0m                                 model.next_word: [next_ind]}\n\u001b[0;32m---> 31\u001b[0;31m                     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                     \u001b[0mtotal_loss\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n = len(corpus)\n",
    "num_epochs = 2\n",
    "model = Model(hidden_size=512, embedding_size=100, vocab_size=20000)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    with tqdm(total=n*num_epochs) as pbar:\n",
    "        for epoch in range(num_epochs):\n",
    "            count = 0\n",
    "            arr = np.arange(n)\n",
    "            np.random.shuffle(arr)\n",
    "            for i in arr:\n",
    "                count+=1\n",
    "                c_init = np.zeros((1, model.rnn.state_size.c), np.float32)\n",
    "                h_init = np.zeros((1, model.rnn.state_size.h), np.float32)\n",
    "                total_loss = 0\n",
    "                for j in range(len(corpus[i])-1):\n",
    "                    word = corpus[i][j]\n",
    "                    next_word = corpus[i][j+1]\n",
    "                    try:\n",
    "                        ind = token2id[word]\n",
    "                    except KeyError as outlier:\n",
    "                        print('The word ' + word + ' doesn\\'t exist in the vocabulary')\n",
    "                        ind = token2id['<unk>']\n",
    "                    try:\n",
    "                        next_ind = token2id[next_word]\n",
    "                    except KeyError as outlier:\n",
    "                        print('The word ' + next_word + ' doesn\\'t exist in the vocabulary')\n",
    "                        next_ind = token2id['<unk>']\n",
    "                    feed_dict = {model.current_word: [ind], model.c_in: c_init, model.h_in: h_init,\n",
    "                                model.next_word: [next_ind]}\n",
    "                    _,loss = sess.run([model.train_op, model.loss], feed_dict)\n",
    "                    total_loss+=loss\n",
    "                    pbar.update(1)\n",
    "                print(\"epoch {}, story {}, loss {}\".format(epoch+1, count, total_loss))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
